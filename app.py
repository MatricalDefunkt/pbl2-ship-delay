# app.py
import flask
from flask import Flask, request, jsonify
import joblib
import pandas as pd
import numpy as np
import os
import time
from datetime import datetime

# Import the helper functions
from utils import calculate_weather_features_from_forecast

# Import the time series forecasting functionality
from time_series import (
    ARIMAForecaster,
    create_arima_endpoint,
    prepare_daily_delay_time_series,
)

# --- Configuration ---
MODEL_DIR = "models"
# !!! IMPORTANT: Define the exact feature names your models expect !!!
# These must match the lists used during training (after preprocessing name generation for one-hot)
# The preprocessor inside the pipeline handles the actual encoding/scaling
# We just need the input DataFrame columns for the pipeline's preprocessor step
EXPECTED_CATEGORICAL_FEATURES = [
    "type",
    "arrival_hour",
    "arrival_dayofweek",
    "arrival_month",
]
EXPECTED_NUMERICAL_FEATURES_BASE = ["teu"]

# Port operational features that might be missing from frontend requests
PORT_OPERATIONAL_FEATURES = [
    "berth_occupancy_rate_at_pred_time",
    "num_at_berth_at_pred_time",
    "num_waiting_entry_at_pred_time",
    "num_waiting_berth_at_pred_time",
]

# Add port operational features to base numerical features
EXPECTED_NUMERICAL_FEATURES_BASE += PORT_OPERATIONAL_FEATURES

# Add weather feature names generated by the util function (derive dynamically or list explicitly)
# Example (Derive names - safer if util changes):
_temp_windows = [6, 12, 24, 48]
_temp_weather_vars = ["wind_speed_knots", "visibility_nm", "wave_height_m"]
_temp_states = ["Fog", "HighWind"]
WEATHER_FEATURE_NAMES = []
for w in _temp_windows:
    for var in _temp_weather_vars:
        for agg in ["mean", "max", "min", "std"]:
            WEATHER_FEATURE_NAMES.append(f"{var}_{agg}_{w}h")
    for state in _temp_states:
        WEATHER_FEATURE_NAMES.append(f"{state}_hours_{w}h")

EXPECTED_NUMERICAL_FEATURES = EXPECTED_NUMERICAL_FEATURES_BASE + WEATHER_FEATURE_NAMES
# --- End of Configuration ---

# --- Calculate Default Port Operational Values By Hour ---
# Dictionary to store average values by hour of day for each port operational feature
port_operational_defaults_by_hour = {}

# Try to load the training data to calculate realistic defaults
try:
    # Check if synthetic data exists - you may need to adjust the path
    synthetic_data_path = "synthetic_operations_log.csv"
    if os.path.exists(synthetic_data_path):
        print(
            f"Loading synthetic data from {synthetic_data_path} to calculate port operational defaults..."
        )

        # Load the data
        df = pd.read_csv(synthetic_data_path)

        # If timestamp column exists, extract hour
        if "timestamp" in df.columns:
            # Convert to datetime if it's not already
            if not pd.api.types.is_datetime64_any_dtype(df["timestamp"]):
                df["timestamp"] = pd.to_datetime(df["timestamp"])

            # Extract hour
            df["hour"] = df["timestamp"].dt.hour

            # Calculate average values for each port operational feature by hour
            for hour in range(24):
                hour_data = df[df["hour"] == hour]
                hour_defaults = {}

                for feature in PORT_OPERATIONAL_FEATURES:
                    if feature in df.columns:
                        hour_defaults[feature] = hour_data[feature].mean()
                    else:
                        # Default values if feature not in data
                        if "rate" in feature:
                            hour_defaults[feature] = 0.7  # 70% default occupancy rate
                        else:
                            hour_defaults[feature] = 5.0  # Default count of 5 vessels

                port_operational_defaults_by_hour[hour] = hour_defaults

            print("Successfully calculated port operational defaults by hour.")
        else:
            raise ValueError("Timestamp column not found in synthetic data")
    else:
        raise FileNotFoundError(f"Synthetic data file {synthetic_data_path} not found")

except Exception as e:
    print(f"Could not calculate port operational defaults from data: {e}")
    print("Using hard-coded default values instead.")

    # Use hard-coded defaults by hour if data loading fails
    for hour in range(24):
        # Vary defaults slightly by hour to capture daily patterns
        multiplier = 1.0 + 0.2 * np.sin(hour * np.pi / 12)  # Peaks at midday

        port_operational_defaults_by_hour[hour] = {
            "berth_occupancy_rate_at_pred_time": 0.7 * multiplier,  # 70% base occupancy
            "num_at_berth_at_pred_time": 5.0 * multiplier,  # 5 vessels at berth
            "num_waiting_entry_at_pred_time": 3.0
            * multiplier,  # 3 vessels waiting entry
            "num_waiting_berth_at_pred_time": 2.0
            * multiplier,  # 2 vessels waiting berth
        }
    print("Created default port operational values by hour.")

# Default values without hour differentiation as fallback
DEFAULT_PORT_OPERATIONAL_VALUES = {
    "berth_occupancy_rate_at_pred_time": 0.7,  # 70% occupancy
    "num_at_berth_at_pred_time": 5.0,  # 5 vessels at berth
    "num_waiting_entry_at_pred_time": 3.0,  # 3 vessels waiting entry
    "num_waiting_berth_at_pred_time": 2.0,  # 2 vessels waiting berth
}

# --- Load Models On Startup ---
models = {}
print("Loading models...")
try:
    for filename in os.listdir(MODEL_DIR):
        if filename.endswith(".joblib"):
            model_name = (
                filename.replace("_pipeline.joblib", "").replace("_", " ").title()
            )
            model_path = os.path.join(MODEL_DIR, filename)
            print(f"Loading model: {model_name} from {model_path}")
            # Ensure you handle potential exceptions during loading
            try:
                models[model_name] = joblib.load(model_path)
                print(f"Loaded {model_name} successfully.")
            except Exception as load_error:
                print(
                    f"Error loading model {model_name} from {model_path}: {load_error}"
                )
    if not models:
        print("Warning: No models loaded. Check the 'models' directory.")
except FileNotFoundError:
    print(f"Error: Model directory '{MODEL_DIR}' not found.")
except Exception as e:
    print(f"An unexpected error occurred during model loading: {e}")


# --- Create Flask App ---
app = Flask(__name__)

# --- Initialize ARIMA Forecaster ---
try:
    # Try to load the ARIMA forecaster if statsmodels is available
    arima_forecaster = ARIMAForecaster(model_dir=MODEL_DIR)
    # Try to load any pre-trained ARIMA models
    arima_model_loaded = arima_forecaster.load_model()
    sarima_model_loaded = arima_forecaster.load_model(seasonal=True)

    if arima_model_loaded:
        models["ARIMA"] = "ARIMA time series forecaster"
    if sarima_model_loaded:
        models["SARIMA"] = "Seasonal ARIMA time series forecaster"

    # If there's no pre-trained model yet, we'll still provide the endpoint
    # The models will be trained when needed or loaded after training
    if not (arima_model_loaded or sarima_model_loaded):
        print("No pre-trained ARIMA models found. They will be trained when needed.")

    # Add ARIMA forecaster to models dictionary for reference
    models["arima_forecaster"] = arima_forecaster

except ImportError:
    print("statsmodels not available - ARIMA forecasting will be disabled")
except Exception as e:
    print(f"Error initializing ARIMA forecaster: {e}")

# Register ARIMA endpoints
create_arima_endpoint(app, models)


# --- API Endpoints ---
@app.route("/", methods=["GET"])
def health_check():
    """Basic health check endpoint."""
    available_models = list(models.keys())
    return (
        jsonify(
            {
                "status": "OK",
                "message": "Prediction server is running.",
                "available_models": available_models,
            }
        ),
        200,
    )


@app.route("/predict", methods=["POST"])
def predict():
    """Endpoint to make delay predictions."""
    start_time = time.time()

    if not request.is_json:
        return jsonify({"error": "Request must be JSON"}), 400

    data = request.get_json()

    # --- 1. Input Validation ---
    required_fields = [
        "vessel_type",
        "teu",
        "arrival_timestamp_str",
        "hourly_weather_forecast",
        "model_name",
    ]
    if not all(field in data for field in required_fields):
        return (
            jsonify({"error": f"Missing required fields. Required: {required_fields}"}),
            400,
        )

    model_name = data["model_name"]
    if model_name not in models:
        return (
            jsonify(
                {
                    "error": f"Model '{model_name}' not found. Available models: {list(models.keys())}"
                }
            ),
            404,
        )

    # --- 2. Feature Preparation ---
    try:
        # Basic vessel and time features
        vessel_type = data["vessel_type"]
        teu = float(data["teu"])
        arrival_ts_str = data["arrival_timestamp_str"]
        # Attempt to parse timestamp (handle potential errors)
        try:
            # Try common ISO formats, add more as needed
            arrival_ts = pd.to_datetime(arrival_ts_str)
        except ValueError:
            return (
                jsonify(
                    {
                        "error": "Invalid format for 'arrival_timestamp_str'. Use ISO 8601 format (e.g., YYYY-MM-DDTHH:MM:SSZ)."
                    }
                ),
                400,
            )

        arrival_hour = arrival_ts.hour
        arrival_dayofweek = arrival_ts.dayofweek
        arrival_month = arrival_ts.month

        # Weather features (calculated by helper function)
        hourly_forecast = data["hourly_weather_forecast"]  # List of dicts
        weather_features = calculate_weather_features_from_forecast(
            arrival_ts, hourly_forecast
        )

        if weather_features.empty or weather_features.isnull().all():
            return (
                jsonify(
                    {
                        "error": "Could not calculate weather features from the provided forecast data. Check forecast format and coverage."
                    }
                ),
                400,
            )

        # --- 3. Assemble Input DataFrame for Model ---
        # Create a dictionary matching the expected feature names
        input_data = {
            "type": [vessel_type],
            "teu": [teu],
            "arrival_hour": [arrival_hour],
            "arrival_dayofweek": [arrival_dayofweek],
            "arrival_month": [arrival_month],
        }

        # Add port operational features - use defaults if not provided
        for feature in PORT_OPERATIONAL_FEATURES:
            if feature in data:
                # Use provided value if available
                input_data[feature] = [float(data[feature])]
            else:
                # Get hour-specific default value
                try:
                    default_value = port_operational_defaults_by_hour[arrival_hour][
                        feature
                    ]
                    print(
                        f"Using default value for {feature} at hour {arrival_hour}: {default_value}"
                    )
                except (KeyError, TypeError):
                    # Fallback to general default if hour-specific not available
                    default_value = DEFAULT_PORT_OPERATIONAL_VALUES[feature]
                    print(f"Using general default value for {feature}: {default_value}")

                input_data[feature] = [default_value]

        # Add weather features dynamically
        for feature_name in WEATHER_FEATURE_NAMES:
            if feature_name in weather_features:
                input_data[feature_name] = [weather_features[feature_name]]
            else:
                # Handle missing weather features (e.g., a state never occurred in forecast) - fill with 0 or NaN? 0 is often safer.
                print(
                    f"Warning: Weather feature '{feature_name}' not generated by util function. Using 0."
                )
                input_data[feature_name] = [0.0]

        # Define the full expected feature list for the DataFrame
        all_expected_features = (
            EXPECTED_CATEGORICAL_FEATURES + EXPECTED_NUMERICAL_FEATURES
        )

        # Create DataFrame with columns in the correct order expected by the preprocessor
        try:
            input_df = pd.DataFrame(input_data, columns=all_expected_features)
            # Ensure correct dtypes where possible (though pipeline should handle)
            input_df[EXPECTED_NUMERICAL_FEATURES] = input_df[
                EXPECTED_NUMERICAL_FEATURES
            ].astype(float)
            input_df[EXPECTED_CATEGORICAL_FEATURES] = input_df[
                EXPECTED_CATEGORICAL_FEATURES
            ].astype(
                str
            )  # OneHotEncoder expects strings/objects
        except KeyError as e:
            return (
                jsonify(
                    {
                        "error": f"Internal error: Missing expected feature column during DataFrame creation: {e}. Check feature definitions."
                    }
                ),
                500,
            )

        # --- 4. Make Prediction ---
        selected_pipeline = models[model_name]
        prediction_raw = selected_pipeline.predict(input_df)

        # Ensure prediction is a single value and non-negative
        predicted_delay = max(
            0, float(prediction_raw[0])
        )  # Extract first element and ensure >= 0

    except ValueError as ve:
        # Catch specific errors like invalid float conversion
        return jsonify({"error": f"Invalid input data value: {ve}"}), 400
    except Exception as e:
        # Catch-all for other unexpected errors during processing/prediction
        print(f"An error occurred during prediction: {e}")
        import traceback

        traceback.print_exc()  # Log detailed error for debugging
        return (
            jsonify({"error": "An internal server error occurred during prediction."}),
            500,
        )

    # --- 5. Format Response ---
    end_time = time.time()
    response = {
        "model_used": model_name,
        "input_data_summary": {  # Provide a summary, not the full raw forecast
            "vessel_type": vessel_type,
            "teu": teu,
            "arrival_timestamp": arrival_ts.isoformat(),
            "port_defaults_used": [
                f for f in PORT_OPERATIONAL_FEATURES if f not in data
            ],
        },
        "predicted_total_weather_delay_hrs": round(predicted_delay, 4),
        "processing_time_seconds": round(end_time - start_time, 4),
    }

    return jsonify(response), 200


# --- Run the App ---
if __name__ == "__main__":
    # Host='0.0.0.0' makes it accessible on your network, not just localhost
    # Debug=True provides auto-reloading and more error details (DO NOT USE IN PRODUCTION)
    app.run(host="0.0.0.0", port=5000, debug=True)
